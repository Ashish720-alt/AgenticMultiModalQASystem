A key consideration when applying reinforcement learning (RL) to a physical setting is the risk and expense of running trials, e.g., while learning the optimal policy for a robot. Another consideration is the robustness of the learned policies. Since it is typically infeasible to test a policy in all contexts, it is difficult to ensure it works as broadly as intended. Fortunately, policies can often be tested in a simulator that exposes key environment variables â€“ state features that are unobserved and randomly determined by the environment in a physical setting but are controllable in the simulator. This paper considers how to use environment variables to help learn robust policies.

Although running trials in a simulator is cheaper and safer than running physical trials, the computational cost of each simulated trial can still be quite high. The challenge then is to develop algorithms that are sample efficient, i.e., that minimise the number of such trials. In such settings, Bayesian Optimisation (BO) (?) is a sample-efficient approach that has been successfully applied to RL in multiple domains (?;Â ?;Â ?;Â ?;Â ?).

A naÃ¯ve approach would be to randomly sample values for the environment variables in each trial, so as to estimate expected performance. However, this approach (1) often requires testing each policy in a prohibitively large number of scenarios, and (2) is not robust to significant rare events (SREs), i.e., it fails any time there are rare events that substantially affect expected performance. For example, rare localisation errors may mean that a robot is much nearer to an obstacle than expected, increasing the risk of a collision. Since collisions are so catastrophic, avoiding them is key to maximising expected performance, even though the factors contributing to the collision occur only rarely. In such cases, the naÃ¯ve approach will not see such rare events often enough to learn an appropriate response.

Instead, we propose a new approach called alternating optimisation and quadrature (ALOQ) specifically aimed towards learning policies that are robust to these rare events while being as sample efficient as possible. The main idea is to actively select the environment variables (instead of sampling them) in a simulator thanks to a Gaussian Process (GP) that models returns as a function of both the policy and the environment variables and then, at each time-step, to use BO and Bayesian Quadrature (BQ) in turn to select a policy and environment setting, respectively, to evaluate.

We apply ALOQ to a number of problems and our results demonstrate that ALOQ learns better and faster than multiple baselines. We also demonstrate that the policy learnt by ALOQ in a simulated hexapod transfers successfully to the real robot.

? (?) also consider the problems posed by SREs. In particular, they propose an approach based on importance sampling (IS) for efficiently evaluating policies whose expected value may be substantially affected by rare events. While their approach is based on temporal difference (TD) methods, we take a BO-based policy search approach. Unlike TD methods, BO is well suited to settings in which sample efficiency is paramount and/or where assumptions (e.g., the Markov property) that underlie TD methods cannot be verified. More importantly, they assume prior knowledge of the SREs, such that they can directly alter the probability of such events during policy evaluation. By contrast, a key strength of ALOQ is that it requires only that a set of environment variables can be controlled in the simulator, without assuming any prior knowledge about whether SREs exist, or about the settings of the environment variables that might trigger them.

More recently, ? (?) also proposed an IS based algorithm, OFFER, where the setting of the environment variable is gradually changed based on observed trials. Since OFFER is a TD method, it suffers from all the disadvantages mentioned earlier. It also assumes that the environment variable only affects the initial state as otherwise it leads to unstable IS estimates.

? (?) consider a problem setting they call the design of computer experiments that is similar to our setting, but does not specifically consider SREs. Their proposed GP-based approach marginalises out the environment variable by alternating between BO and BQ. However, unlike ALOQ, their method is based on the EI acquisition function, which makes it computationally expensive for reasons discussed in Section 4, and is applicable only to discrete environment variables. We include their method as a baseline in our experiments. Our results presented in Section 5 show that, compared to ALOQ, their method is unsuitable for settings with SREs. Further, their method is far more computationally expensive and fails even to outperform a baseline that randomly samples the environment variable at each step.

? (?) also address optimising performance in the presence of environment variables. However, they address a fundamentally different contextual bandit setting in which the learned policy conditions on the observed environment variable.

PILCO (?) is a model-based policy search method that achieves remarkable sample efficiency in robot control (?). PILCO superficially resembles ALOQ in its use of GPs but the key difference is that in PILCO the GP models the transition dynamics while in ALOQ it models the returns as a function of the policy and environment variable. PILCO is fundamentally ill suited to our setting. First, it assumes that the transition dynamics are Gaussian and can be learned with a few hundred observed transitions, which is often infeasible in more complex environments (i.e., it scales poorly as the dimensionality of the state/action space increases). Second, even in simple environments, PILCO will not be able to learn the transition dynamics because in our setting the environment variable is not observed in physical trials, leading to major violations of the Gaussian assumption when those environment variables can cause SREs.

Policies found in simulators are rarely optimal when deployed on the physical agent due to the reality gap that may exist due to the inability of any simulator to model reality perfectly. EPOpt (?) tries to address this by finding policies that are robust to simulators with different settings of its parameters. First, multiple instances of the simulator are generated by drawing a random sample of the simulator parameter settings. Trajectories are then sampled from each of these instances and used by a batch policy optimisation algorithm (e.g. TRPO (?)). While ALOQ finds a risk-neutral policy, EPOpt finds a risk-averse solution based on maximising the conditional value at risk (CVaR) by feeding the policy optimisation only the sampled trajectories whose returns are lower than the CVaR. In a risk-neutral setting, EPOpt reduces to the underlying policy optimisation algorithm with trajectories randomly sampled from different instances of the simulator. This approach will not see SREs often enough to learn an appropriate response, as we demonstrate in our experiments.

? (?) also suggest a method to address the problem of finding robust policies. Their method learns a policy by training in a simulator that is adversarial in nature, i.e., the simulator settings are dynamically chosen to minimise the returns of the policy. This method requires significant prior knowledge to be able to set the simulator settings such that it provides just the right amount of challenge to the policy. Furthermore, it does not consider any settings with SREs.

GPs provide a principled way of quantifying uncertainties associated with modelling unknown functions. A GP is a distribution over functions, and is fully specified by its mean function mâ€‹(ğ±)ğ‘šğ±m(\mathbf{x}) and covariance function kâ€‹(ğ±,ğ±â€²)ğ‘˜ğ±superscriptğ±â€²k(\mathbf{x},\mathbf{x^{\prime}}) (see ? (?) for an in-depth treatment) which encode any prior belief about the nature of the function. The prior can be combined with observed values to update the belief about the function in a Bayesian way to generate a posterior distribution.

The prior mean function of the GP is often assumed to be 0 for convenience. A popular choice for the covariance function is the class of stationary functions of the form kâ€‹(ğ±,ğ±â€²)=kâ€‹(ğ±âˆ’ğ±â€²)ğ‘˜ğ±superscriptğ±â€²ğ‘˜ğ±superscriptğ±â€²k(\mathbf{x},\mathbf{x^{\prime}})=k(\mathbf{x}-\mathbf{x^{\prime}}), which implies that the correlation between the function values of any two points depends only on the distance between them.

In GP regression, it is assumed that the observed function values {fâ€‹(ğ±i)}i=1Nsuperscriptsubscriptğ‘“subscriptğ±ğ‘–ğ‘–1ğ‘\{f(\mathbf{x}_{i})\}_{i=1}^{N} is a sample from a multivariate Gaussian distribution. The prediction for a new point ğ±âˆ—superscriptğ±\mathbf{x}^{*} is connected with the observations through the mean and covariance functions. By conditioning on the observed data, this can be computed analytically as a Gaussian ğ’©â€‹(Î¼â€‹(fâ€‹(ğ±âˆ—)),Ïƒ2â€‹(fâ€‹(ğ±âˆ—)))ğ’©ğœ‡ğ‘“superscriptğ±superscriptğœ2ğ‘“superscriptğ±\mathcal{N}\bigl{(}\mu\bigl{(}f(\mathbf{x}^{*})\bigr{)},\sigma^{2}\bigl{(}f(\mathbf{x}^{*})\bigr{)}\bigr{)}:Î¼â€‹(fâ€‹(ğ±âˆ—))ğœ‡ğ‘“superscriptğ±\displaystyle\mu\bigl{(}f(\mathbf{x}^{*})\bigr{)}=kâ€‹(ğ±âˆ—,ğ—)â€‹(ğŠ+Ïƒnâ€‹oâ€‹iâ€‹sâ€‹e2â€‹ğˆ)âˆ’1â€‹fâ€‹(ğ—)absentğ‘˜superscriptğ±ğ—superscriptğŠsubscriptsuperscriptğœ2ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ğˆ1ğ‘“ğ—\displaystyle=k(\mathbf{x}^{*},\mathbf{X})(\mathbf{K}+\sigma^{2}_{noise}\mathbf{I})^{-1}f(\mathbf{X})(1a)Ïƒ2â€‹(fâ€‹(ğ±âˆ—))superscriptğœ2ğ‘“superscriptğ±\displaystyle\sigma^{2}\bigl{(}f(\mathbf{x}^{*})\bigr{)}=kâ€‹(ğ±âˆ—,ğ±âˆ—)absentğ‘˜superscriptğ±superscriptğ±\displaystyle=k(\mathbf{x}^{*},\mathbf{x}^{*})(1b)âˆ’kâ€‹(ğ±âˆ—,ğ—)â€‹(ğŠ+Ïƒnâ€‹oâ€‹iâ€‹sâ€‹e2â€‹ğˆ)âˆ’1â€‹kâ€‹(ğ—,ğ±âˆ—),ğ‘˜superscriptğ±ğ—superscriptğŠsubscriptsuperscriptğœ2ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ğˆ1ğ‘˜ğ—superscriptğ±\displaystyle-k(\mathbf{x}^{*},\mathbf{X})(\mathbf{K}+\sigma^{2}_{noise}\mathbf{I})^{-1}k(\mathbf{X},\mathbf{x}^{*}),(1c)where ğ—ğ—\mathbf{X} denotes the vector of observed inputs, fâ€‹(ğ—)ğ‘“ğ—f(\mathbf{X}) the vector of corresponding function values, and ğŠğŠ\mathbf{K} is the matrix with entries kâ€‹(ğ±ğ¢,ğ±ğ£)ğ‘˜subscriptğ±ğ¢subscriptğ±ğ£k(\mathbf{x_{i}},\mathbf{x_{j}}).

This property of generating estimates of the uncertainty associated with any prediction makes it particularly suited for finding the optimum of fâ€‹(ğ±)ğ‘“ğ±f(\mathbf{x}) using BO. BO requires an acquisition function to guide the search and balance exploitation (searching the space expected to have the optimum) and exploration (searching the space which has not been explored well). Given a set of observations, the next point for evaluation is actively chosen as the ğ±ğ±\mathbf{x} that maximises the acquisition function.

Two commonly used acquisition functions are expected improvement (EI) (?;Â ?) and upper confidence bound (UCB) (?;Â ?). Defining ğ±+superscriptğ±\mathbf{x}^{+} as the current optimal evaluation, i.e., ğ±+=argâ¡maxğ±iâ¡fâ€‹(ğ±i)superscriptğ±subscriptsubscriptğ±ğ‘–ğ‘“subscriptğ±ğ‘–\mathbf{x}^{+}=\operatorname*{\arg\!\max}_{\mathbf{x}_{i}}f(\mathbf{x}_{i}), EI seeks to maximise the expected improvement over the current optimum Î±Eâ€‹Iâ€‹(ğ±)=ğ”¼â€‹[Iâ€‹(ğ±)]subscriptğ›¼ğ¸ğ¼ğ±ğ”¼delimited-[]ğ¼ğ±\alpha_{EI}(\mathbf{x})=\mathbb{E}[I(\mathbf{x})], where Iâ€‹(ğ±)=maxâ¡{0,fâ€‹(ğ±)âˆ’fâ€‹(ğ±+)}ğ¼ğ±0ğ‘“ğ±ğ‘“superscriptğ±I(\mathbf{x})=\max\{0,f(\mathbf{x})-f(\mathbf{x^{+}})\}. By contrast, UCB does not depend on ğ±+superscriptğ±\mathbf{x}^{+} but directly incorporates the uncertainty in the prediction by defining an upper bound: Î±Uâ€‹Câ€‹Bâ€‹(ğ±)=Î¼â€‹(ğ±)+Îºâ€‹Ïƒâ€‹(ğ±)subscriptğ›¼ğ‘ˆğ¶ğµğ±ğœ‡ğ±ğœ…ğœğ±\alpha_{UCB}(\mathbf{x})=\mu(\mathbf{x})+\kappa\sigma(\mathbf{x}), where Îºğœ…\kappa controls the exploration-exploitation tradeoff.

BQ (?;Â ?) is a sample-efficient technique for computing integrals of the form fÂ¯=âˆ«fâ€‹(ğ±)â€‹pâ€‹(ğ±)â€‹ğ‘‘ğ±Â¯ğ‘“ğ‘“ğ±ğ‘ğ±differential-dğ±\bar{f}=\int f(\mathbf{x})p(\mathbf{x})d\mathbf{x}, where pâ€‹(ğ±)ğ‘ğ±p(\mathbf{x}) is a probability distribution. Using GP regression to compute the prediction for any fâ€‹(ğ±)ğ‘“ğ±f(\mathbf{x}) given some observed data, fÂ¯Â¯ğ‘“\bar{f} is a Gaussian whose mean and variance can be computed analytically for particular choices of the covariance function and pâ€‹(ğ±)ğ‘ğ±p(\mathbf{x}) (?). If no analytical solution exists, we can approximate the mean and variance via Monte Carlo quadrature by sampling the predictions of various fâ€‹(ğ±)ğ‘“ğ±f(\mathbf{x}).

Given some observed data ğ’Ÿğ’Ÿ\mathcal{D}, we can also devise acquisition functions for BQ to actively select the next point ğ±âˆ—superscriptğ±\mathbf{x^{*}} for evaluation. A natural objective here is to select ğ±ğ±\mathbf{x} that minimises the uncertainty of fÂ¯Â¯ğ‘“\bar{f}, i.e., ğ±âˆ—=argâ¡minğ±â¡ğ•â€‹(fÂ¯|ğ’Ÿ,ğ±)superscriptğ±subscriptğ±ğ•conditionalÂ¯ğ‘“ğ’Ÿğ±\mathbf{x^{*}}=\operatorname*{\arg\!\min}_{\mathbf{x}}\mathbb{V}(\bar{f}|\mathcal{D},\mathbf{x}) (?). Due to the nature of GPs, ğ•â€‹(fÂ¯|ğ’Ÿ,ğ±)ğ•conditionalÂ¯ğ‘“ğ’Ÿğ±\mathbb{V}(\bar{f}|\mathcal{D},\mathbf{x}) does not depend on fâ€‹(ğ±)ğ‘“ğ±f(\mathbf{x}) and is thus computationally feasible to evaluate. Uncertainty sampling (?) is an alternative acquisition function that chooses the ğ±âˆ—superscriptğ±\mathbf{x^{*}} with the maximum posterior variance:ğ±âˆ—=argâ¡maxğ±â¡ğ•â€‹(fâ€‹(ğ±)|ğ’Ÿ)superscriptğ±subscriptğ±ğ•conditionalğ‘“ğ±ğ’Ÿ\mathbf{x^{*}}=\operatorname*{\arg\!\max}_{\mathbf{x}}\mathbb{V}(f(\mathbf{x})|\mathcal{D}). Although simple and computationally cheap, it is not the same as reducing uncertainty about fÂ¯Â¯ğ‘“\bar{f} since evaluating the point with the highest prediction uncertainty does not necessarily lead to the maximum reduction in the uncertainty of the estimate of the integral.

Monte Carlo (MC) quadrature simply samples (ğ±1,ğ±2,â€¦,ğ±N)subscriptğ±1subscriptğ±2â€¦subscriptğ±ğ‘(\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{N}) from pâ€‹(ğ±)ğ‘ğ±p(\mathbf{x}) and estimates the integral as fÂ¯â‰ˆ1Nâ€‹âˆ‘i=1Nfâ€‹(ğ±i).Â¯ğ‘“1ğ‘superscriptsubscriptğ‘–1ğ‘ğ‘“subscriptğ±ğ‘–\bar{f}\approx\frac{1}{N}\sum_{i=1}^{N}f(\mathbf{x}_{i}). This typically requires a large Nğ‘N and so is less sample efficient than BQ: it should only be used if fğ‘“f is cheap to evaluate.The many merits of BQ over MC, both philosophically and practically, are brought out by ? (?) and ? (?).Below, we will describe an active Bayesian quadrature scheme (that is, selecting points according to an acquisition function), inspired by the empirical improvements offered by those of ? (?) and ? (?).

We assume access to a computationally expensive simulator that takes as input a policy Ï€âˆˆğ’œğœ‹ğ’œ\pi\in\mathcal{A} and environment variable Î¸âˆˆâ„¬ğœƒâ„¬\theta\in\mathcal{B} and produces as output the return fâ€‹(Ï€,Î¸)âˆˆâ„ğ‘“ğœ‹ğœƒâ„f(\pi,\theta)\in\mathbb{R}, where both ğ’œğ’œ\mathcal{A} and â„¬â„¬\mathcal{B} belong to some compact sets in â„dÏ€superscriptâ„subscriptğ‘‘ğœ‹\mathbb{R}^{d_{\pi}} and â„dÎ¸superscriptâ„subscriptğ‘‘ğœƒ\mathbb{R}^{d_{\theta}}, respectively.

We also assume access to pâ€‹(Î¸)ğ‘ğœƒp(\theta), the probability distribution over Î¸ğœƒ\theta. pâ€‹(Î¸)ğ‘ğœƒp(\theta) may be known a priori, or it may be a posterior distribution estimated from whatever physical trials have been conducted. Note that we do not require a perfect simulator: any uncertainty about the dynamics of the physical world can be modelled in pâ€‹(Î¸)ğ‘ğœƒp(\theta), i.e., some environment variables may just be simulator parameters whose correct fixed setting is not known with certainty.

Defining fi=fâ€‹(Ï€i,Î¸i)subscriptğ‘“ğ‘–ğ‘“subscriptğœ‹ğ‘–subscriptğœƒğ‘–f_{i}=f(\pi_{i},\theta_{i}), we assume we have a dataset ğ’Ÿ1:l={(Ï€1,Î¸1,f1),(Ï€2,Î¸2,f2),â€¦,(Ï€l,Î¸l,fl)}subscriptğ’Ÿ:1ğ‘™subscriptğœ‹1subscriptğœƒ1subscriptğ‘“1subscriptğœ‹2subscriptğœƒ2subscriptğ‘“2â€¦subscriptğœ‹ğ‘™subscriptğœƒğ‘™subscriptğ‘“ğ‘™\mathcal{D}_{1:l}=\{(\pi_{1},\theta_{1},f_{1}),(\pi_{2},\theta_{2},f_{2}),\ldots,(\pi_{l},\theta_{l},f_{l})\}. Our objective is to find an optimal policy Ï€âˆ—superscriptğœ‹\pi^{*}:Ï€âˆ—=argâ¡maxÏ€â¡fÂ¯â€‹(Ï€)=argâ¡maxÏ€â¡ğ”¼Î¸â€‹[fâ€‹(Ï€,Î¸)].superscriptğœ‹subscriptğœ‹Â¯ğ‘“ğœ‹subscriptğœ‹subscriptğ”¼ğœƒdelimited-[]ğ‘“ğœ‹ğœƒ\displaystyle\pi^{*}=\operatorname*{\arg\!\max}_{\pi}\bar{f}(\pi)=\operatorname*{\arg\!\max}_{\pi}\mathbb{E}_{\theta}[f(\pi,\theta)].(2)

First, consider a naÃ¯ve approach consisting of a standard application of BO that disregards Î¸ğœƒ\theta, performs BO on f~â€‹(Ï€)=fâ€‹(Ï€,Î¸)~ğ‘“ğœ‹ğ‘“ğœ‹ğœƒ\tilde{f}(\pi)=f(\pi,\theta) with only one input Ï€ğœ‹\pi, and attempts to estimate Ï€âˆ—superscriptğœ‹\pi^{*}. Formally, this approach models f~~ğ‘“\tilde{f} as a GP with a zero mean function and a suitable covariance function kâ€‹(Ï€,Ï€â€²)ğ‘˜ğœ‹superscriptğœ‹â€²k(\pi,\pi^{\prime}). For any given Ï€ğœ‹\pi, the variation in fğ‘“f due to different settings of Î¸ğœƒ\theta is treated as noise. To estimate Ï€âˆ—superscriptğœ‹\pi^{*}, the naÃ¯ve approach applies BO, while sampling Î¸ğœƒ\theta from pâ€‹(Î¸)ğ‘ğœƒp(\theta) at each timestep. This approach will almost surely fail due to not sampling SREs often enough to learn a suitable response.

By contrast, our method ALOQ (see Alg.Â 1) models fâ€‹(Ï€,Î¸)ğ‘“ğœ‹ğœƒf(\pi,\theta) as a GP: fâˆ¼Gâ€‹Pâ€‹(m,k)similar-toğ‘“ğºğ‘ƒğ‘šğ‘˜f\sim GP(m,k), acknowledging both its inputs. The main idea behind ALOQ is, given ğ’Ÿ1:lsubscriptğ’Ÿ:1ğ‘™\mathcal{D}_{1:l}, to use a BO acquisition function to select Ï€l+1subscriptğœ‹ğ‘™1\pi_{l+1} for evaluation and then use a BQ acquisition function to select Î¸l+1subscriptğœƒğ‘™1\theta_{l+1}, conditioning on Ï€l+1subscriptğœ‹ğ‘™1\pi_{l+1}.

Selecting Ï€l+1subscriptğœ‹ğ‘™1\pi_{l+1} requires maximising a BO acquisition function (6) on fÂ¯â€‹(Ï€)Â¯ğ‘“ğœ‹\bar{f}(\pi), which requires estimating fÂ¯â€‹(Ï€)Â¯ğ‘“ğœ‹\bar{f}(\pi), together with the uncertainty associated with it. Fortunately BQ is well suited for this since it can use the GP to estimate fÂ¯â€‹(Ï€)Â¯ğ‘“ğœ‹\bar{f}(\pi) together with the uncertainty associated with it. This is illustrated in Figure 1.

Once Ï€l+1subscriptğœ‹ğ‘™1\pi_{l+1} is chosen, ALOQ selects Î¸l+1subscriptğœƒğ‘™1\theta_{l+1} by minimising a BQ acquisition function (7) quantifying the uncertainty about fÂ¯â€‹(Ï€l+1)Â¯ğ‘“subscriptğœ‹ğ‘™1\bar{f}(\pi_{l+1}). After (Ï€t+1,Î¸l+1)subscriptğœ‹ğ‘¡1subscriptğœƒğ‘™1(\pi_{t+1},\theta_{l+1}) is selected, ALOQ evaluates it on the simulator and updates the GP with the new datapoint (Ï€l+1,Î¸l+1,fl+1)subscriptğœ‹ğ‘™1subscriptğœƒğ‘™1subscriptğ‘“ğ‘™1(\pi_{l+1},\theta_{l+1},f_{l+1}). Our estimate of Ï€âˆ—superscriptğœ‹\pi^{*} is thus:Ï€^âˆ—=argâ¡maxÏ€â¡ğ”¼â€‹[fÂ¯â€‹(Ï€)|ğ’Ÿ1:l+1].superscript^ğœ‹subscriptğœ‹ğ”¼delimited-[]conditionalÂ¯ğ‘“ğœ‹subscriptğ’Ÿ:1ğ‘™1\displaystyle\hat{\pi}^{*}=\operatorname*{\arg\!\max}_{\pi}\mathbb{E}[\bar{f}(\pi)|\mathcal{D}_{1:l+1}].(3)Although the approach described so far actively selects Ï€ğœ‹\pi and Î¸ğœƒ\theta through BO and BQ, it is unlikely to perform well in practice. A key observation is that the presence of SREs, which we seek to address with ALOQ, implies that the scale of fğ‘“f varies considerably, e.g., returns in case of collision vs no collision. This nonstationarity cannot be modelled with our stationary kernel. Therefore, we must transform the inputs to ensure stationarity of fğ‘“f. In particular, we employ Beta warping, i.e., transform the inputs using Beta CDFs with parameters (Î±,Î²)ğ›¼ğ›½(\mathbf{\alpha},\mathbf{\beta}) (?). The CDF of the beta distribution on the support 0<x<10ğ‘¥10<x<1 is given by:BetaCDFâ€‹(x,Î±,Î²)=âˆ«0xuÎ±âˆ’1â€‹(1âˆ’u)Î²âˆ’1Bâ€‹(Î±,Î²)â€‹dâ€‹u,BetaCDFğ‘¥ğ›¼ğ›½superscriptsubscript0ğ‘¥superscriptğ‘¢ğ›¼1superscript1ğ‘¢ğ›½1ğµğ›¼ğ›½dğ‘¢\text{BetaCDF}(x,\alpha,\beta)=\int_{0}^{x}\frac{u^{\alpha-1}(1-u)^{\beta-1}}{B(\alpha,\beta)}\text{d}u,(4)where Bâ€‹(Î±,Î²)ğµğ›¼ğ›½B(\alpha,\beta) is the beta function. The beta CDF is particularly suitable for our purpose as it is able to model a variety of warpings based on the settings of only two parameters (Î±,Î²)ğ›¼ğ›½(\alpha,\beta). ALOQ transforms each dimension of Ï€ğœ‹\pi and Î¸ğœƒ\theta independently, and treats the corresponding (Î±,Î²)ğ›¼ğ›½(\alpha,\beta) as hyperparameters. We assume that we are working with the transformed inputs for the rest of the paper.

While the resulting algorithm should be able to cope with SREs, the Ï€^âˆ—superscript^ğœ‹\hat{\pi}^{*} that it returns at each iteration may still be poor, since our BQ evaluation of fÂ¯â€‹(Ï€)Â¯ğ‘“ğœ‹\bar{f}(\pi) leads to a noisy approximation of the true expected return. This is particularly problematic in high dimensional settings. To address this, intensification (?;Â ?), i.e., re-evaluation of selected policies in the simulator, is essential. Therefore, ALOQ performs two simulator calls at each timestep. In the first evaluation, (Ï€l+1,Î¸l+1)subscriptğœ‹ğ‘™1subscriptğœƒğ‘™1(\pi_{l+1},\theta_{l+1}) is selected via the BO/BQ scheme described earlier. In the second stage, (Ï€^âˆ—,Î¸âˆ—)superscript^ğœ‹superscriptğœƒ(\hat{\pi}^{*},\theta^{*}) is evaluated, where Ï€^âˆ—âˆˆÏ€1:l+1superscript^ğœ‹subscriptğœ‹:1ğ‘™1\hat{\pi}^{*}\in\pi_{1:l+1} is selected using (3) and Î¸âˆ—|Ï€^âˆ—conditionalsuperscriptğœƒsuperscript^ğœ‹\theta^{*}|\hat{\pi}^{*} using the BQ acquisition function (7).

Computing fÂ¯â€‹(Ï€)Â¯ğ‘“ğœ‹\bar{f}(\pi): For discrete Î¸ğœƒ\theta with support {Î¸1,Î¸2,â€¦,Î¸NÎ¸}subscriptğœƒ1subscriptğœƒ2â€¦subscriptğœƒsubscriptğ‘ğœƒ\{\theta_{1},\theta_{2},\ldots,\theta_{N_{\theta}}\}, the estimate of the mean Î¼ğœ‡\mu and variance Ïƒ2superscriptğœ2\sigma^{2} for fÂ¯â€‹(Ï€)âˆ£ğ’Ÿ1:lconditionalÂ¯ğ‘“ğœ‹subscriptğ’Ÿ:1ğ‘™\bar{f}(\pi)\mid\mathcal{D}_{1:l} is straightforward:Î¼ğœ‡\displaystyle\mu=1NÎ¸â€‹âˆ‘i=1NÎ¸ğ”¼â€‹[fâ€‹(Ï€,Î¸i)|ğ’Ÿ1:l]absent1subscriptğ‘ğœƒsuperscriptsubscriptğ‘–1subscriptğ‘ğœƒğ”¼delimited-[]conditionalğ‘“ğœ‹subscriptğœƒğ‘–subscriptğ’Ÿ:1ğ‘™\displaystyle=\frac{1}{N_{\theta}}\sum_{i=1}^{N_{\theta}}\mathbb{E}[f(\pi,\theta_{i})|\mathcal{D}_{1:l}](5a)Ïƒ2superscriptğœ2\displaystyle\sigma^{2}=1NÎ¸2âˆ‘i=1NÎ¸âˆ‘j=1NÎ¸Cov[f(Ï€,Î¸i)|ğ’Ÿ1:l,f(Ï€,Î¸j)|ğ’Ÿ1:l],\displaystyle=\frac{1}{N_{\theta}^{2}}\sum_{i=1}^{N_{\theta}}\sum_{j=1}^{N_{\theta}}Cov[f(\pi,\theta_{i})|\mathcal{D}_{1:l},f(\pi,\theta_{j})|\mathcal{D}_{1:l}],(5b)where fâ€‹(Ï€,Î¸)ğ‘“ğœ‹ğœƒf(\pi,\theta) is the prediction from the GP with mean and covariance computed using (1). For continuous Î¸ğœƒ\theta, we apply Monte Carlo quadrature. Although this requires sampling a large number of Î¸ğœƒ\theta and evaluating the corresponding fâ€‹(Ï€,Î¸)âˆ£ğ’Ÿ1:lconditionalğ‘“ğœ‹ğœƒsubscriptğ’Ÿ:1ğ‘™f(\pi,\theta)\mid\mathcal{D}_{1:l}, it is feasible since we evaluate fâ€‹(Ï€,Î¸)âˆ£ğ’Ÿ1:lconditionalğ‘“ğœ‹ğœƒsubscriptğ’Ÿ:1ğ‘™f(\pi,\theta)\mid\mathcal{D}_{1:l}, not from the expensive simulator, but from the computationally cheaper GP.

BO acquisition function for Ï€ğœ‹\pi: A modified version of the UCB acquisition function is a natural choice since using (5) we can compute it easily asÎ±Aâ€‹Lâ€‹Oâ€‹Qâ€‹(Ï€)=Î¼â€‹(fÂ¯â€‹(Ï€)âˆ£ğ’Ÿ1:l)+Îºâ€‹Ïƒâ€‹(fÂ¯â€‹(Ï€)âˆ£ğ’Ÿ1:l),subscriptğ›¼ğ´ğ¿ğ‘‚ğ‘„ğœ‹ğœ‡conditionalÂ¯ğ‘“ğœ‹subscriptğ’Ÿ:1ğ‘™ğœ…ğœconditionalÂ¯ğ‘“ğœ‹subscriptğ’Ÿ:1ğ‘™\displaystyle\alpha_{ALOQ}(\pi)=\mu(\bar{f}(\pi)\mid\mathcal{D}_{1:l})+\kappa\sigma(\bar{f}(\pi)\mid\mathcal{D}_{1:l}),(6)and set Ï€l+1=argâ¡maxÏ€â¡Î±Aâ€‹Lâ€‹Oâ€‹Qâ€‹(Ï€)subscriptğœ‹ğ‘™1subscriptğœ‹subscriptğ›¼ğ´ğ¿ğ‘‚ğ‘„ğœ‹\pi_{l+1}=\operatorname*{\arg\!\max}_{\pi}\alpha_{ALOQ}(\pi).

Note that although it is possible to define an EI-based acquisition function: Î±=ğ”¼fÂ¯â€‹(Ï€)|ğ’Ÿ1:lâ€‹[Iâ€‹(Ï€)]ğ›¼subscriptğ”¼conditionalÂ¯ğ‘“ğœ‹subscriptğ’Ÿ:1ğ‘™delimited-[]ğ¼ğœ‹\alpha=\mathbb{E}_{\bar{f}(\pi)|\mathcal{D}_{1:l}}[I(\pi)], where Iâ€‹(Ï€)=maxâ¡{0,fÂ¯â€‹(Ï€)âˆ’fÂ¯â€‹(Ï€+)}ğ¼ğœ‹0Â¯ğ‘“ğœ‹Â¯ğ‘“superscriptğœ‹I(\pi)=\max\{0,\bar{f}(\pi)-\bar{f}(\pi^{+})\}, as an alternative choice for ALOQ, it is prohibitively expensive to compute in practice. The stochastic fÂ¯â€‹(Ï€+)âˆ£ğ’Ÿ1:lconditionalÂ¯ğ‘“superscriptğœ‹subscriptğ’Ÿ:1ğ‘™\bar{f}(\pi^{+})\mid\mathcal{D}_{1:l} renders this analytically intractable. Approximating it using Monte Carlo sampling would require performing predictions on lÃ—NÎ¸ğ‘™subscriptğ‘ğœƒl\times N_{\theta} points, i.e., all the lğ‘™l observed Ï€ğœ‹\piâ€™s paired with all the NÎ¸subscriptğ‘ğœƒN_{\theta} possible settings of the environment variable, which is infeasible even for moderate lğ‘™l as the computational complexity of GP predictions scales quadratically with the number of predictions.

BQ acquisition function for Î¸ğœƒ\theta: BQ can be viewed as performing policy evaluation in our approach. Since the presence of SREs leads to high variance in the returns associated with any given policy, it is of critical importance that we minimise the uncertainty associated with our estimate of the expected return of a policy. We formalise this objective through our BQ acquisition function for Î¸ğœƒ\theta: ALOQ selects Î¸l+1âˆ£Ï€l+1conditionalsubscriptğœƒğ‘™1subscriptğœ‹ğ‘™1\theta_{l+1}\mid\pi_{l+1} by minimising the posterior variance of fÂ¯â€‹(Ï€l+1)Â¯ğ‘“subscriptğœ‹ğ‘™1\bar{f}(\pi_{l+1}), yielding:Î¸l+1|Ï€l+1=argâ¡minÎ¸â¡ğ•â€‹(fÂ¯â€‹(Ï€l+1)|ğ’Ÿ1:l,Ï€l+1,Î¸).conditionalsubscriptğœƒğ‘™1subscriptğœ‹ğ‘™1subscriptğœƒğ•conditionalÂ¯ğ‘“subscriptğœ‹ğ‘™1subscriptğ’Ÿ:1ğ‘™subscriptğœ‹ğ‘™1ğœƒ\displaystyle\theta_{l+1}|\pi_{l+1}=\operatorname*{\arg\!\min}_{\theta}\mathbb{V}(\bar{f}(\pi_{l+1})|\mathcal{D}_{1:l},\pi_{l+1},\theta).(7)We also tried uncertainty sampling in our experiments. Unsurprisingly it performed worse as it is not as good at reducing the uncertainty associated with the expected return of a policy as explained in Section 3.

Properties of ALOQ: Thanks to convergence guarantees for BO using Î±Uâ€‹Câ€‹Bsubscriptğ›¼ğ‘ˆğ¶ğµ\alpha_{UCB} (?), ALOQ converges if the BQ scheme on which it relies also converges. Unfortunately, to the best of our knowledge, existing convergence guarantees (?;Â ?) apply only to BQ methods that do not actively select points, as (7) does.Of course, we expect such active selection to only improve the rate of convergence of our algorithms over non-active versions.However, our empirical results in Section 5 show that in practice ALOQ efficiently optimises policies in the presence of SREs across a variety of tasks.

ALOQâ€™s computational complexity is dominated by an ğ’ªâ€‹(l3)ğ’ªsuperscriptğ‘™3\mathcal{O}(l^{3}) matrix inversion, where lğ‘™l is the sample size of the dataset ğ’Ÿğ’Ÿ\mathcal{D}. This cubic scaling is common to all BO methods involving GPs. The BQ integral estimation in each iteration requires only GP predictions, which are ğ’ªâ€‹(l2)ğ’ªsuperscriptğ‘™2\mathcal{O}(l^{2}).

To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where pâ€‹(Î¸)ğ‘ğœƒp(\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task (?). Further experiments on test functions to clearly show the how each element of ALOQ is necessary for settings with SREs is presented in the supplementary material.

We compare ALOQ to several baselines: 1) the naÃ¯ve method described in the previous section; 2) the method of ? (?), which we refer to as WSN; 3) the simple policy gradient method Reinforce (?), and 4) the state-of-the-art policy gradient method TRPO (?). To show the importance of each component of ALOQ, we also perform experiments with ablated versions of ALOQ, namely: 1) Random Quadrature ALOQ (RQ-ALOQ), in which Î¸ğœƒ\theta is sampled randomly from pâ€‹(Î¸)ğ‘ğœƒp(\theta) instead of being chosen actively; 2) unwarped ALOQ, which does not perform Beta warping of the inputs; and 3) one-step ALOQ, which does not use intensification. All plotted results are the median of 20 independent runs. Details of the experimental setups and the variability in performance can be found in the supplementary material.

In this experiment, we evaluate ALOQâ€™s performance on a robot control problem implemented in a kinematic simulator. The goal is to configure each of the three controllable joints of a robot arm such that the tip of the arm gets as close as possible to a predefined target point.

In the first setting, we assume that the robotic arm is part of a mobile robot that has localised itself near the target. However, due to localisation errors, there is a small possibility that it is near a wall and some joint angles may lead to the arm colliding with the wall and incurring a large cost. Minimising cost entails getting as close to the target as possible while avoiding the region where the wall may be present. The environment variable in this setting is the distance to the wall.

Figures 2(a) and 2(b) show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all.

Figure 2(c) shows the learned arm configurations, as well as the policy that would be learned by ALOQ if there was no wall (No Wall). The shaded region represents the possible locations of the wall. This plot illustrates that ALOQ learns a policy that gets closest to the target. Furthermore, while all the BO based algorithms learn to avoid the wall, active selection of Î¸ğœƒ\theta allows ALOQ to do so more quickly: smart quadrature allows it to more efficiently observe rare events and accurately estimate their boundary. For readability we have only presented the arm configurations for algorithms which have performance comparable to ALOQ.

Next we consider a variation in which instead of uncertainty introduced by localisation, some settings of the first joint carry a 5% probability of it breaking, which consequently incurs a large cost. Minimising cost thus entails getting as close to the target as possible, while minimising the probability of the joint breaking.

Figures 3(a) and 3(b) shows the expected cost (lower is better) of the arm configurations after each timestep for each method. Since Î¸ğœƒ\theta is continuous in this setting, and WSN requires discrete Î¸ğœƒ\theta, it was run on a slightly different version with Î¸ğœƒ\theta discretised by 100 equidistant points. The results are similar to the previous experiment, except that the baselines perform worse. In particular, the NaÃ¯ve baseline, WSN, and Reinforce seem to have converged to a suboptimal policy since they have not witnessed any SREs.

Figure 3(c) shows the learned arm configurations together with the policy that would be learned if there were no SREs (â€˜No breakâ€™). The shaded region represents the joint angles that can lead to failure. This figure illustrates that ALOQ learns a qualitatively different policy than the other algorithms, one which avoids the joint angles that might lead to a breakage while still getting close to the target faster than the other methods. Again for readability we only present the arm configurations for the most competitive algorithms.

Both these baselines are relatively sample inefficient. However, one question that arises is whether these methods eventually find the optimal policy. To check this, we ran them for 2000 iterations with a batch size of 5 trajectories (thus a total of 10000 simulator calls). We repeated this for both the Collision Avoidance and Joint Breakage settings. The expected cost of the arm configurations after each iteration are presented in Figure 4 (we only present the results up to 1000 simulator calls for readability - there is no improvement beyond what can be seen in the plot). Both baselines can solve the tasks in settings without SREs, i.e. where there is no possibility of a collision or a breakage (â€™No Wallâ€™ and â€™No Breakâ€™ in the figures). However, in settings with SREs they converge rapidly to a suboptimal policy from which they are unable recover even if run for much longer, since they donâ€™t experience the SREs often enough. This is especially striking in the collision avoidance task where TRPO converges to a policy that has a relatively high probability of leading to a collision.

Now we consider the setting where pâ€‹(Î¸)ğ‘ğœƒp(\theta) is not known a priori, but must be approximated using trajectories from some baseline policy. In this setting, instead of directly setting the robot armâ€™s joint angles, we set the torque applied to each joint (Ï€)ğœ‹(\pi). The final joint angles are determined by the torque and the unknown friction between the joints (Î¸)ğœƒ(\theta). Setting the torque too high can lead to the joint breaking, which incurs a large cost.

We use the simulator as a proxy for both real trials as well as the simulated trials. In the first case, we simply sample Î¸ğœƒ\theta from a uniform prior, run a baseline policy, and use the observed returns to compute an approximate posterior over Î¸ğœƒ\theta. We then use ALOQ to compute the optimal policy over this posterior (â€˜ALOQ policyâ€™). For comparison, we also compute the MAP of Î¸ğœƒ\theta and the corresponding optimal policy (â€˜MAP policyâ€™). To show that active selection of Î¸ğœƒ\theta is advantageous, we also compare against the policy learned by RQ-ALOQ.

Since we are approximating the unknown pâ€‹(Î¸)ğ‘ğœƒp(\theta) with a set of samples, it makes sense to keep the sample size relatively low for computational efficiency when finding the ALOQ policy (50 samples in this instance). However, to show that ALOQ is robust to this approximation, when comparing the performance of the ALOQ and MAP policies, we used a much larger sample size of 400 for the posterior distribution.

For evaluation, we drew 1000 samples of Î¸ğœƒ\theta from the more granular posterior distribution and measured the returns of the three policies for each of the samples. The average cost incurred by the ALOQ policy (presented in Table 1) was 31% lower than that incurred by the MAP policy and 23.6% lower than the RQ-ALOQ policy. This is because ALOQ finds a policy that slightly underperforms the MAP policy in some of cases but avoids over 95% of the SREs (cost â‰¥\geq70 in Table 1) experienced by the MAP and RQ-ALOQ policies.

As robots move from fully controlled environments to more complex and natural ones, they have to face the inevitable risk of getting damaged. However, it may be expensive or even impossible to decommission a robot whenever any damage condition prevents it from completing its task. Hence, it is desirable to develop methods that enable robots to recover from failure.

Intelligent trial and error (IT&E) (?) has been shown to recover from various damage conditions and thereby prevent catastrophic failure. Before deployment, IT&E uses the simulator to create an archive of diverse and locally high performing policies for the intact robot that are mapped to a lower dimensional behaviour space. If the robot becomes damaged after deployment, it uses BO to quickly find the policy in the archive that has the highest performance on the damaged robot. However, it can only respond after damage has occurred. Though it learns quickly, performance may still be poor while learning during the initial trials after damage occurs. To mitigate this effect, we propose to use ALOQ to learn in simulation the policy with the highest expected performance across the possible damage conditions. By deploying this policy, instead of the policy that is optimal for the intact robot, we can minimise in expectation the negative effects of damage in the period before IT&E has learned to recover.

We consider a hexapod locomotion task with a setup similar to that of (?) to demonstrate this experimentally. The objective is to cross a finish line a fixed distance from its starting point. Failure to cross the line leads to a large negative reward, while the reward for completing the task is inversely proportional to the time taken.

It is possible that a subset of the legs may be damaged or broken when deployed in a physical setting. For our experiments we assume that, based on prior experience, any of the front two or back two legs can be shortened or removed with probability of 10% and 5% respectively, independent of the other legs, leading to 81 possible configurations. We excluded the middle two legs from our experiment as their failure had a relatively lower impact on the hexapodâ€™s movement. The configuration of the six legs acts as our environment variable. Figure 5(a) shows one such setting.

We applied ALOQ to learn the optimal policy given these damage probabilities, but restricted the search to the policies in the archive created by (?). Figure 5(b) shows that ALOQ finds a policy with much higher expected reward than RQ-ALOQ. It also shows the policy that generates the maximum reward when none of the legs are damaged or broken (â€˜opt undamaged policyâ€™).

To demonstrate that ALOQ learns a policy that can be applied to a physical environment, we also deployed the best ALOQ policy on the real hexapod. In order to limit the number of physical trials required to evaluate ALOQ, we limited the possibility of damage to the rear two legs. The learnt policy performed well on the physical robot because it optimised performance on the rare configurations that matter most for expected return (e.g., either leg shortened).

This paper proposed ALOQ, a novel approach to using BO and BQ to perform sample-efficient RL in a way that is robust to the presence of significant rare events. We empirically evaluated ALOQ on different simulated tasks involving a robotic arm simulator, and a hexapod locomotion taskand showed how it can be also be applied to settings where the distribution of the environment variable is unknown a priori, and that it successfully transfers to a real robot. Our results demonstrated that ALOQ outperforms multiple baselines, including related methods proposed in the literature. Further, ALOQ is computationally efficient and does not require any restrictive assumptions to be made about the environment variables.