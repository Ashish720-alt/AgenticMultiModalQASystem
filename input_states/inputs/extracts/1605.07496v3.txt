A key consideration when applying reinforcement learning (RL) to a physical setting is the risk and expense of running trials, e.g., while learning the optimal policy for a robot. Another consideration is the robustness of the learned policies. Since it is typically infeasible to test a policy in all contexts, it is difficult to ensure it works as broadly as intended. Fortunately, policies can often be tested in a simulator that exposes key environment variables – state features that are unobserved and randomly determined by the environment in a physical setting but are controllable in the simulator. This paper considers how to use environment variables to help learn robust policies.

Although running trials in a simulator is cheaper and safer than running physical trials, the computational cost of each simulated trial can still be quite high. The challenge then is to develop algorithms that are sample efficient, i.e., that minimise the number of such trials. In such settings, Bayesian Optimisation (BO) (?) is a sample-efficient approach that has been successfully applied to RL in multiple domains (?; ?; ?; ?; ?).

A naïve approach would be to randomly sample values for the environment variables in each trial, so as to estimate expected performance. However, this approach (1) often requires testing each policy in a prohibitively large number of scenarios, and (2) is not robust to significant rare events (SREs), i.e., it fails any time there are rare events that substantially affect expected performance. For example, rare localisation errors may mean that a robot is much nearer to an obstacle than expected, increasing the risk of a collision. Since collisions are so catastrophic, avoiding them is key to maximising expected performance, even though the factors contributing to the collision occur only rarely. In such cases, the naïve approach will not see such rare events often enough to learn an appropriate response.

Instead, we propose a new approach called alternating optimisation and quadrature (ALOQ) specifically aimed towards learning policies that are robust to these rare events while being as sample efficient as possible. The main idea is to actively select the environment variables (instead of sampling them) in a simulator thanks to a Gaussian Process (GP) that models returns as a function of both the policy and the environment variables and then, at each time-step, to use BO and Bayesian Quadrature (BQ) in turn to select a policy and environment setting, respectively, to evaluate.

We apply ALOQ to a number of problems and our results demonstrate that ALOQ learns better and faster than multiple baselines. We also demonstrate that the policy learnt by ALOQ in a simulated hexapod transfers successfully to the real robot.

? (?) also consider the problems posed by SREs. In particular, they propose an approach based on importance sampling (IS) for efficiently evaluating policies whose expected value may be substantially affected by rare events. While their approach is based on temporal difference (TD) methods, we take a BO-based policy search approach. Unlike TD methods, BO is well suited to settings in which sample efficiency is paramount and/or where assumptions (e.g., the Markov property) that underlie TD methods cannot be verified. More importantly, they assume prior knowledge of the SREs, such that they can directly alter the probability of such events during policy evaluation. By contrast, a key strength of ALOQ is that it requires only that a set of environment variables can be controlled in the simulator, without assuming any prior knowledge about whether SREs exist, or about the settings of the environment variables that might trigger them.

More recently, ? (?) also proposed an IS based algorithm, OFFER, where the setting of the environment variable is gradually changed based on observed trials. Since OFFER is a TD method, it suffers from all the disadvantages mentioned earlier. It also assumes that the environment variable only affects the initial state as otherwise it leads to unstable IS estimates.

? (?) consider a problem setting they call the design of computer experiments that is similar to our setting, but does not specifically consider SREs. Their proposed GP-based approach marginalises out the environment variable by alternating between BO and BQ. However, unlike ALOQ, their method is based on the EI acquisition function, which makes it computationally expensive for reasons discussed in Section 4, and is applicable only to discrete environment variables. We include their method as a baseline in our experiments. Our results presented in Section 5 show that, compared to ALOQ, their method is unsuitable for settings with SREs. Further, their method is far more computationally expensive and fails even to outperform a baseline that randomly samples the environment variable at each step.

? (?) also address optimising performance in the presence of environment variables. However, they address a fundamentally different contextual bandit setting in which the learned policy conditions on the observed environment variable.

PILCO (?) is a model-based policy search method that achieves remarkable sample efficiency in robot control (?). PILCO superficially resembles ALOQ in its use of GPs but the key difference is that in PILCO the GP models the transition dynamics while in ALOQ it models the returns as a function of the policy and environment variable. PILCO is fundamentally ill suited to our setting. First, it assumes that the transition dynamics are Gaussian and can be learned with a few hundred observed transitions, which is often infeasible in more complex environments (i.e., it scales poorly as the dimensionality of the state/action space increases). Second, even in simple environments, PILCO will not be able to learn the transition dynamics because in our setting the environment variable is not observed in physical trials, leading to major violations of the Gaussian assumption when those environment variables can cause SREs.

Policies found in simulators are rarely optimal when deployed on the physical agent due to the reality gap that may exist due to the inability of any simulator to model reality perfectly. EPOpt (?) tries to address this by finding policies that are robust to simulators with different settings of its parameters. First, multiple instances of the simulator are generated by drawing a random sample of the simulator parameter settings. Trajectories are then sampled from each of these instances and used by a batch policy optimisation algorithm (e.g. TRPO (?)). While ALOQ finds a risk-neutral policy, EPOpt finds a risk-averse solution based on maximising the conditional value at risk (CVaR) by feeding the policy optimisation only the sampled trajectories whose returns are lower than the CVaR. In a risk-neutral setting, EPOpt reduces to the underlying policy optimisation algorithm with trajectories randomly sampled from different instances of the simulator. This approach will not see SREs often enough to learn an appropriate response, as we demonstrate in our experiments.

? (?) also suggest a method to address the problem of finding robust policies. Their method learns a policy by training in a simulator that is adversarial in nature, i.e., the simulator settings are dynamically chosen to minimise the returns of the policy. This method requires significant prior knowledge to be able to set the simulator settings such that it provides just the right amount of challenge to the policy. Furthermore, it does not consider any settings with SREs.

GPs provide a principled way of quantifying uncertainties associated with modelling unknown functions. A GP is a distribution over functions, and is fully specified by its mean function m​(𝐱)𝑚𝐱m(\mathbf{x}) and covariance function k​(𝐱,𝐱′)𝑘𝐱superscript𝐱′k(\mathbf{x},\mathbf{x^{\prime}}) (see ? (?) for an in-depth treatment) which encode any prior belief about the nature of the function. The prior can be combined with observed values to update the belief about the function in a Bayesian way to generate a posterior distribution.

The prior mean function of the GP is often assumed to be 0 for convenience. A popular choice for the covariance function is the class of stationary functions of the form k​(𝐱,𝐱′)=k​(𝐱−𝐱′)𝑘𝐱superscript𝐱′𝑘𝐱superscript𝐱′k(\mathbf{x},\mathbf{x^{\prime}})=k(\mathbf{x}-\mathbf{x^{\prime}}), which implies that the correlation between the function values of any two points depends only on the distance between them.

In GP regression, it is assumed that the observed function values {f​(𝐱i)}i=1Nsuperscriptsubscript𝑓subscript𝐱𝑖𝑖1𝑁\{f(\mathbf{x}_{i})\}_{i=1}^{N} is a sample from a multivariate Gaussian distribution. The prediction for a new point 𝐱∗superscript𝐱\mathbf{x}^{*} is connected with the observations through the mean and covariance functions. By conditioning on the observed data, this can be computed analytically as a Gaussian 𝒩​(μ​(f​(𝐱∗)),σ2​(f​(𝐱∗)))𝒩𝜇𝑓superscript𝐱superscript𝜎2𝑓superscript𝐱\mathcal{N}\bigl{(}\mu\bigl{(}f(\mathbf{x}^{*})\bigr{)},\sigma^{2}\bigl{(}f(\mathbf{x}^{*})\bigr{)}\bigr{)}:μ​(f​(𝐱∗))𝜇𝑓superscript𝐱\displaystyle\mu\bigl{(}f(\mathbf{x}^{*})\bigr{)}=k​(𝐱∗,𝐗)​(𝐊+σn​o​i​s​e2​𝐈)−1​f​(𝐗)absent𝑘superscript𝐱𝐗superscript𝐊subscriptsuperscript𝜎2𝑛𝑜𝑖𝑠𝑒𝐈1𝑓𝐗\displaystyle=k(\mathbf{x}^{*},\mathbf{X})(\mathbf{K}+\sigma^{2}_{noise}\mathbf{I})^{-1}f(\mathbf{X})(1a)σ2​(f​(𝐱∗))superscript𝜎2𝑓superscript𝐱\displaystyle\sigma^{2}\bigl{(}f(\mathbf{x}^{*})\bigr{)}=k​(𝐱∗,𝐱∗)absent𝑘superscript𝐱superscript𝐱\displaystyle=k(\mathbf{x}^{*},\mathbf{x}^{*})(1b)−k​(𝐱∗,𝐗)​(𝐊+σn​o​i​s​e2​𝐈)−1​k​(𝐗,𝐱∗),𝑘superscript𝐱𝐗superscript𝐊subscriptsuperscript𝜎2𝑛𝑜𝑖𝑠𝑒𝐈1𝑘𝐗superscript𝐱\displaystyle-k(\mathbf{x}^{*},\mathbf{X})(\mathbf{K}+\sigma^{2}_{noise}\mathbf{I})^{-1}k(\mathbf{X},\mathbf{x}^{*}),(1c)where 𝐗𝐗\mathbf{X} denotes the vector of observed inputs, f​(𝐗)𝑓𝐗f(\mathbf{X}) the vector of corresponding function values, and 𝐊𝐊\mathbf{K} is the matrix with entries k​(𝐱𝐢,𝐱𝐣)𝑘subscript𝐱𝐢subscript𝐱𝐣k(\mathbf{x_{i}},\mathbf{x_{j}}).

This property of generating estimates of the uncertainty associated with any prediction makes it particularly suited for finding the optimum of f​(𝐱)𝑓𝐱f(\mathbf{x}) using BO. BO requires an acquisition function to guide the search and balance exploitation (searching the space expected to have the optimum) and exploration (searching the space which has not been explored well). Given a set of observations, the next point for evaluation is actively chosen as the 𝐱𝐱\mathbf{x} that maximises the acquisition function.

Two commonly used acquisition functions are expected improvement (EI) (?; ?) and upper confidence bound (UCB) (?; ?). Defining 𝐱+superscript𝐱\mathbf{x}^{+} as the current optimal evaluation, i.e., 𝐱+=arg⁡max𝐱i⁡f​(𝐱i)superscript𝐱subscriptsubscript𝐱𝑖𝑓subscript𝐱𝑖\mathbf{x}^{+}=\operatorname*{\arg\!\max}_{\mathbf{x}_{i}}f(\mathbf{x}_{i}), EI seeks to maximise the expected improvement over the current optimum αE​I​(𝐱)=𝔼​[I​(𝐱)]subscript𝛼𝐸𝐼𝐱𝔼delimited-[]𝐼𝐱\alpha_{EI}(\mathbf{x})=\mathbb{E}[I(\mathbf{x})], where I​(𝐱)=max⁡{0,f​(𝐱)−f​(𝐱+)}𝐼𝐱0𝑓𝐱𝑓superscript𝐱I(\mathbf{x})=\max\{0,f(\mathbf{x})-f(\mathbf{x^{+}})\}. By contrast, UCB does not depend on 𝐱+superscript𝐱\mathbf{x}^{+} but directly incorporates the uncertainty in the prediction by defining an upper bound: αU​C​B​(𝐱)=μ​(𝐱)+κ​σ​(𝐱)subscript𝛼𝑈𝐶𝐵𝐱𝜇𝐱𝜅𝜎𝐱\alpha_{UCB}(\mathbf{x})=\mu(\mathbf{x})+\kappa\sigma(\mathbf{x}), where κ𝜅\kappa controls the exploration-exploitation tradeoff.

BQ (?; ?) is a sample-efficient technique for computing integrals of the form f¯=∫f​(𝐱)​p​(𝐱)​𝑑𝐱¯𝑓𝑓𝐱𝑝𝐱differential-d𝐱\bar{f}=\int f(\mathbf{x})p(\mathbf{x})d\mathbf{x}, where p​(𝐱)𝑝𝐱p(\mathbf{x}) is a probability distribution. Using GP regression to compute the prediction for any f​(𝐱)𝑓𝐱f(\mathbf{x}) given some observed data, f¯¯𝑓\bar{f} is a Gaussian whose mean and variance can be computed analytically for particular choices of the covariance function and p​(𝐱)𝑝𝐱p(\mathbf{x}) (?). If no analytical solution exists, we can approximate the mean and variance via Monte Carlo quadrature by sampling the predictions of various f​(𝐱)𝑓𝐱f(\mathbf{x}).

Given some observed data 𝒟𝒟\mathcal{D}, we can also devise acquisition functions for BQ to actively select the next point 𝐱∗superscript𝐱\mathbf{x^{*}} for evaluation. A natural objective here is to select 𝐱𝐱\mathbf{x} that minimises the uncertainty of f¯¯𝑓\bar{f}, i.e., 𝐱∗=arg⁡min𝐱⁡𝕍​(f¯|𝒟,𝐱)superscript𝐱subscript𝐱𝕍conditional¯𝑓𝒟𝐱\mathbf{x^{*}}=\operatorname*{\arg\!\min}_{\mathbf{x}}\mathbb{V}(\bar{f}|\mathcal{D},\mathbf{x}) (?). Due to the nature of GPs, 𝕍​(f¯|𝒟,𝐱)𝕍conditional¯𝑓𝒟𝐱\mathbb{V}(\bar{f}|\mathcal{D},\mathbf{x}) does not depend on f​(𝐱)𝑓𝐱f(\mathbf{x}) and is thus computationally feasible to evaluate. Uncertainty sampling (?) is an alternative acquisition function that chooses the 𝐱∗superscript𝐱\mathbf{x^{*}} with the maximum posterior variance:𝐱∗=arg⁡max𝐱⁡𝕍​(f​(𝐱)|𝒟)superscript𝐱subscript𝐱𝕍conditional𝑓𝐱𝒟\mathbf{x^{*}}=\operatorname*{\arg\!\max}_{\mathbf{x}}\mathbb{V}(f(\mathbf{x})|\mathcal{D}). Although simple and computationally cheap, it is not the same as reducing uncertainty about f¯¯𝑓\bar{f} since evaluating the point with the highest prediction uncertainty does not necessarily lead to the maximum reduction in the uncertainty of the estimate of the integral.

Monte Carlo (MC) quadrature simply samples (𝐱1,𝐱2,…,𝐱N)subscript𝐱1subscript𝐱2…subscript𝐱𝑁(\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{N}) from p​(𝐱)𝑝𝐱p(\mathbf{x}) and estimates the integral as f¯≈1N​∑i=1Nf​(𝐱i).¯𝑓1𝑁superscriptsubscript𝑖1𝑁𝑓subscript𝐱𝑖\bar{f}\approx\frac{1}{N}\sum_{i=1}^{N}f(\mathbf{x}_{i}). This typically requires a large N𝑁N and so is less sample efficient than BQ: it should only be used if f𝑓f is cheap to evaluate.The many merits of BQ over MC, both philosophically and practically, are brought out by ? (?) and ? (?).Below, we will describe an active Bayesian quadrature scheme (that is, selecting points according to an acquisition function), inspired by the empirical improvements offered by those of ? (?) and ? (?).

We assume access to a computationally expensive simulator that takes as input a policy π∈𝒜𝜋𝒜\pi\in\mathcal{A} and environment variable θ∈ℬ𝜃ℬ\theta\in\mathcal{B} and produces as output the return f​(π,θ)∈ℝ𝑓𝜋𝜃ℝf(\pi,\theta)\in\mathbb{R}, where both 𝒜𝒜\mathcal{A} and ℬℬ\mathcal{B} belong to some compact sets in ℝdπsuperscriptℝsubscript𝑑𝜋\mathbb{R}^{d_{\pi}} and ℝdθsuperscriptℝsubscript𝑑𝜃\mathbb{R}^{d_{\theta}}, respectively.

We also assume access to p​(θ)𝑝𝜃p(\theta), the probability distribution over θ𝜃\theta. p​(θ)𝑝𝜃p(\theta) may be known a priori, or it may be a posterior distribution estimated from whatever physical trials have been conducted. Note that we do not require a perfect simulator: any uncertainty about the dynamics of the physical world can be modelled in p​(θ)𝑝𝜃p(\theta), i.e., some environment variables may just be simulator parameters whose correct fixed setting is not known with certainty.

Defining fi=f​(πi,θi)subscript𝑓𝑖𝑓subscript𝜋𝑖subscript𝜃𝑖f_{i}=f(\pi_{i},\theta_{i}), we assume we have a dataset 𝒟1:l={(π1,θ1,f1),(π2,θ2,f2),…,(πl,θl,fl)}subscript𝒟:1𝑙subscript𝜋1subscript𝜃1subscript𝑓1subscript𝜋2subscript𝜃2subscript𝑓2…subscript𝜋𝑙subscript𝜃𝑙subscript𝑓𝑙\mathcal{D}_{1:l}=\{(\pi_{1},\theta_{1},f_{1}),(\pi_{2},\theta_{2},f_{2}),\ldots,(\pi_{l},\theta_{l},f_{l})\}. Our objective is to find an optimal policy π∗superscript𝜋\pi^{*}:π∗=arg⁡maxπ⁡f¯​(π)=arg⁡maxπ⁡𝔼θ​[f​(π,θ)].superscript𝜋subscript𝜋¯𝑓𝜋subscript𝜋subscript𝔼𝜃delimited-[]𝑓𝜋𝜃\displaystyle\pi^{*}=\operatorname*{\arg\!\max}_{\pi}\bar{f}(\pi)=\operatorname*{\arg\!\max}_{\pi}\mathbb{E}_{\theta}[f(\pi,\theta)].(2)

First, consider a naïve approach consisting of a standard application of BO that disregards θ𝜃\theta, performs BO on f~​(π)=f​(π,θ)~𝑓𝜋𝑓𝜋𝜃\tilde{f}(\pi)=f(\pi,\theta) with only one input π𝜋\pi, and attempts to estimate π∗superscript𝜋\pi^{*}. Formally, this approach models f~~𝑓\tilde{f} as a GP with a zero mean function and a suitable covariance function k​(π,π′)𝑘𝜋superscript𝜋′k(\pi,\pi^{\prime}). For any given π𝜋\pi, the variation in f𝑓f due to different settings of θ𝜃\theta is treated as noise. To estimate π∗superscript𝜋\pi^{*}, the naïve approach applies BO, while sampling θ𝜃\theta from p​(θ)𝑝𝜃p(\theta) at each timestep. This approach will almost surely fail due to not sampling SREs often enough to learn a suitable response.

By contrast, our method ALOQ (see Alg. 1) models f​(π,θ)𝑓𝜋𝜃f(\pi,\theta) as a GP: f∼G​P​(m,k)similar-to𝑓𝐺𝑃𝑚𝑘f\sim GP(m,k), acknowledging both its inputs. The main idea behind ALOQ is, given 𝒟1:lsubscript𝒟:1𝑙\mathcal{D}_{1:l}, to use a BO acquisition function to select πl+1subscript𝜋𝑙1\pi_{l+1} for evaluation and then use a BQ acquisition function to select θl+1subscript𝜃𝑙1\theta_{l+1}, conditioning on πl+1subscript𝜋𝑙1\pi_{l+1}.

Selecting πl+1subscript𝜋𝑙1\pi_{l+1} requires maximising a BO acquisition function (6) on f¯​(π)¯𝑓𝜋\bar{f}(\pi), which requires estimating f¯​(π)¯𝑓𝜋\bar{f}(\pi), together with the uncertainty associated with it. Fortunately BQ is well suited for this since it can use the GP to estimate f¯​(π)¯𝑓𝜋\bar{f}(\pi) together with the uncertainty associated with it. This is illustrated in Figure 1.

Once πl+1subscript𝜋𝑙1\pi_{l+1} is chosen, ALOQ selects θl+1subscript𝜃𝑙1\theta_{l+1} by minimising a BQ acquisition function (7) quantifying the uncertainty about f¯​(πl+1)¯𝑓subscript𝜋𝑙1\bar{f}(\pi_{l+1}). After (πt+1,θl+1)subscript𝜋𝑡1subscript𝜃𝑙1(\pi_{t+1},\theta_{l+1}) is selected, ALOQ evaluates it on the simulator and updates the GP with the new datapoint (πl+1,θl+1,fl+1)subscript𝜋𝑙1subscript𝜃𝑙1subscript𝑓𝑙1(\pi_{l+1},\theta_{l+1},f_{l+1}). Our estimate of π∗superscript𝜋\pi^{*} is thus:π^∗=arg⁡maxπ⁡𝔼​[f¯​(π)|𝒟1:l+1].superscript^𝜋subscript𝜋𝔼delimited-[]conditional¯𝑓𝜋subscript𝒟:1𝑙1\displaystyle\hat{\pi}^{*}=\operatorname*{\arg\!\max}_{\pi}\mathbb{E}[\bar{f}(\pi)|\mathcal{D}_{1:l+1}].(3)Although the approach described so far actively selects π𝜋\pi and θ𝜃\theta through BO and BQ, it is unlikely to perform well in practice. A key observation is that the presence of SREs, which we seek to address with ALOQ, implies that the scale of f𝑓f varies considerably, e.g., returns in case of collision vs no collision. This nonstationarity cannot be modelled with our stationary kernel. Therefore, we must transform the inputs to ensure stationarity of f𝑓f. In particular, we employ Beta warping, i.e., transform the inputs using Beta CDFs with parameters (α,β)𝛼𝛽(\mathbf{\alpha},\mathbf{\beta}) (?). The CDF of the beta distribution on the support 0<x<10𝑥10<x<1 is given by:BetaCDF​(x,α,β)=∫0xuα−1​(1−u)β−1B​(α,β)​d​u,BetaCDF𝑥𝛼𝛽superscriptsubscript0𝑥superscript𝑢𝛼1superscript1𝑢𝛽1𝐵𝛼𝛽d𝑢\text{BetaCDF}(x,\alpha,\beta)=\int_{0}^{x}\frac{u^{\alpha-1}(1-u)^{\beta-1}}{B(\alpha,\beta)}\text{d}u,(4)where B​(α,β)𝐵𝛼𝛽B(\alpha,\beta) is the beta function. The beta CDF is particularly suitable for our purpose as it is able to model a variety of warpings based on the settings of only two parameters (α,β)𝛼𝛽(\alpha,\beta). ALOQ transforms each dimension of π𝜋\pi and θ𝜃\theta independently, and treats the corresponding (α,β)𝛼𝛽(\alpha,\beta) as hyperparameters. We assume that we are working with the transformed inputs for the rest of the paper.

While the resulting algorithm should be able to cope with SREs, the π^∗superscript^𝜋\hat{\pi}^{*} that it returns at each iteration may still be poor, since our BQ evaluation of f¯​(π)¯𝑓𝜋\bar{f}(\pi) leads to a noisy approximation of the true expected return. This is particularly problematic in high dimensional settings. To address this, intensification (?; ?), i.e., re-evaluation of selected policies in the simulator, is essential. Therefore, ALOQ performs two simulator calls at each timestep. In the first evaluation, (πl+1,θl+1)subscript𝜋𝑙1subscript𝜃𝑙1(\pi_{l+1},\theta_{l+1}) is selected via the BO/BQ scheme described earlier. In the second stage, (π^∗,θ∗)superscript^𝜋superscript𝜃(\hat{\pi}^{*},\theta^{*}) is evaluated, where π^∗∈π1:l+1superscript^𝜋subscript𝜋:1𝑙1\hat{\pi}^{*}\in\pi_{1:l+1} is selected using (3) and θ∗|π^∗conditionalsuperscript𝜃superscript^𝜋\theta^{*}|\hat{\pi}^{*} using the BQ acquisition function (7).

Computing f¯​(π)¯𝑓𝜋\bar{f}(\pi): For discrete θ𝜃\theta with support {θ1,θ2,…,θNθ}subscript𝜃1subscript𝜃2…subscript𝜃subscript𝑁𝜃\{\theta_{1},\theta_{2},\ldots,\theta_{N_{\theta}}\}, the estimate of the mean μ𝜇\mu and variance σ2superscript𝜎2\sigma^{2} for f¯​(π)∣𝒟1:lconditional¯𝑓𝜋subscript𝒟:1𝑙\bar{f}(\pi)\mid\mathcal{D}_{1:l} is straightforward:μ𝜇\displaystyle\mu=1Nθ​∑i=1Nθ𝔼​[f​(π,θi)|𝒟1:l]absent1subscript𝑁𝜃superscriptsubscript𝑖1subscript𝑁𝜃𝔼delimited-[]conditional𝑓𝜋subscript𝜃𝑖subscript𝒟:1𝑙\displaystyle=\frac{1}{N_{\theta}}\sum_{i=1}^{N_{\theta}}\mathbb{E}[f(\pi,\theta_{i})|\mathcal{D}_{1:l}](5a)σ2superscript𝜎2\displaystyle\sigma^{2}=1Nθ2∑i=1Nθ∑j=1NθCov[f(π,θi)|𝒟1:l,f(π,θj)|𝒟1:l],\displaystyle=\frac{1}{N_{\theta}^{2}}\sum_{i=1}^{N_{\theta}}\sum_{j=1}^{N_{\theta}}Cov[f(\pi,\theta_{i})|\mathcal{D}_{1:l},f(\pi,\theta_{j})|\mathcal{D}_{1:l}],(5b)where f​(π,θ)𝑓𝜋𝜃f(\pi,\theta) is the prediction from the GP with mean and covariance computed using (1). For continuous θ𝜃\theta, we apply Monte Carlo quadrature. Although this requires sampling a large number of θ𝜃\theta and evaluating the corresponding f​(π,θ)∣𝒟1:lconditional𝑓𝜋𝜃subscript𝒟:1𝑙f(\pi,\theta)\mid\mathcal{D}_{1:l}, it is feasible since we evaluate f​(π,θ)∣𝒟1:lconditional𝑓𝜋𝜃subscript𝒟:1𝑙f(\pi,\theta)\mid\mathcal{D}_{1:l}, not from the expensive simulator, but from the computationally cheaper GP.

BO acquisition function for π𝜋\pi: A modified version of the UCB acquisition function is a natural choice since using (5) we can compute it easily asαA​L​O​Q​(π)=μ​(f¯​(π)∣𝒟1:l)+κ​σ​(f¯​(π)∣𝒟1:l),subscript𝛼𝐴𝐿𝑂𝑄𝜋𝜇conditional¯𝑓𝜋subscript𝒟:1𝑙𝜅𝜎conditional¯𝑓𝜋subscript𝒟:1𝑙\displaystyle\alpha_{ALOQ}(\pi)=\mu(\bar{f}(\pi)\mid\mathcal{D}_{1:l})+\kappa\sigma(\bar{f}(\pi)\mid\mathcal{D}_{1:l}),(6)and set πl+1=arg⁡maxπ⁡αA​L​O​Q​(π)subscript𝜋𝑙1subscript𝜋subscript𝛼𝐴𝐿𝑂𝑄𝜋\pi_{l+1}=\operatorname*{\arg\!\max}_{\pi}\alpha_{ALOQ}(\pi).

Note that although it is possible to define an EI-based acquisition function: α=𝔼f¯​(π)|𝒟1:l​[I​(π)]𝛼subscript𝔼conditional¯𝑓𝜋subscript𝒟:1𝑙delimited-[]𝐼𝜋\alpha=\mathbb{E}_{\bar{f}(\pi)|\mathcal{D}_{1:l}}[I(\pi)], where I​(π)=max⁡{0,f¯​(π)−f¯​(π+)}𝐼𝜋0¯𝑓𝜋¯𝑓superscript𝜋I(\pi)=\max\{0,\bar{f}(\pi)-\bar{f}(\pi^{+})\}, as an alternative choice for ALOQ, it is prohibitively expensive to compute in practice. The stochastic f¯​(π+)∣𝒟1:lconditional¯𝑓superscript𝜋subscript𝒟:1𝑙\bar{f}(\pi^{+})\mid\mathcal{D}_{1:l} renders this analytically intractable. Approximating it using Monte Carlo sampling would require performing predictions on l×Nθ𝑙subscript𝑁𝜃l\times N_{\theta} points, i.e., all the l𝑙l observed π𝜋\pi’s paired with all the Nθsubscript𝑁𝜃N_{\theta} possible settings of the environment variable, which is infeasible even for moderate l𝑙l as the computational complexity of GP predictions scales quadratically with the number of predictions.

BQ acquisition function for θ𝜃\theta: BQ can be viewed as performing policy evaluation in our approach. Since the presence of SREs leads to high variance in the returns associated with any given policy, it is of critical importance that we minimise the uncertainty associated with our estimate of the expected return of a policy. We formalise this objective through our BQ acquisition function for θ𝜃\theta: ALOQ selects θl+1∣πl+1conditionalsubscript𝜃𝑙1subscript𝜋𝑙1\theta_{l+1}\mid\pi_{l+1} by minimising the posterior variance of f¯​(πl+1)¯𝑓subscript𝜋𝑙1\bar{f}(\pi_{l+1}), yielding:θl+1|πl+1=arg⁡minθ⁡𝕍​(f¯​(πl+1)|𝒟1:l,πl+1,θ).conditionalsubscript𝜃𝑙1subscript𝜋𝑙1subscript𝜃𝕍conditional¯𝑓subscript𝜋𝑙1subscript𝒟:1𝑙subscript𝜋𝑙1𝜃\displaystyle\theta_{l+1}|\pi_{l+1}=\operatorname*{\arg\!\min}_{\theta}\mathbb{V}(\bar{f}(\pi_{l+1})|\mathcal{D}_{1:l},\pi_{l+1},\theta).(7)We also tried uncertainty sampling in our experiments. Unsurprisingly it performed worse as it is not as good at reducing the uncertainty associated with the expected return of a policy as explained in Section 3.

Properties of ALOQ: Thanks to convergence guarantees for BO using αU​C​Bsubscript𝛼𝑈𝐶𝐵\alpha_{UCB} (?), ALOQ converges if the BQ scheme on which it relies also converges. Unfortunately, to the best of our knowledge, existing convergence guarantees (?; ?) apply only to BQ methods that do not actively select points, as (7) does.Of course, we expect such active selection to only improve the rate of convergence of our algorithms over non-active versions.However, our empirical results in Section 5 show that in practice ALOQ efficiently optimises policies in the presence of SREs across a variety of tasks.

ALOQ’s computational complexity is dominated by an 𝒪​(l3)𝒪superscript𝑙3\mathcal{O}(l^{3}) matrix inversion, where l𝑙l is the sample size of the dataset 𝒟𝒟\mathcal{D}. This cubic scaling is common to all BO methods involving GPs. The BQ integral estimation in each iteration requires only GP predictions, which are 𝒪​(l2)𝒪superscript𝑙2\mathcal{O}(l^{2}).

To evaluate ALOQ we applied it to 1) a simulated robot arm control task, including a variation where p​(θ)𝑝𝜃p(\theta) is not known a priori but must be inferred from data, and 2) a hexapod locomotion task (?). Further experiments on test functions to clearly show the how each element of ALOQ is necessary for settings with SREs is presented in the supplementary material.

We compare ALOQ to several baselines: 1) the naïve method described in the previous section; 2) the method of ? (?), which we refer to as WSN; 3) the simple policy gradient method Reinforce (?), and 4) the state-of-the-art policy gradient method TRPO (?). To show the importance of each component of ALOQ, we also perform experiments with ablated versions of ALOQ, namely: 1) Random Quadrature ALOQ (RQ-ALOQ), in which θ𝜃\theta is sampled randomly from p​(θ)𝑝𝜃p(\theta) instead of being chosen actively; 2) unwarped ALOQ, which does not perform Beta warping of the inputs; and 3) one-step ALOQ, which does not use intensification. All plotted results are the median of 20 independent runs. Details of the experimental setups and the variability in performance can be found in the supplementary material.

In this experiment, we evaluate ALOQ’s performance on a robot control problem implemented in a kinematic simulator. The goal is to configure each of the three controllable joints of a robot arm such that the tip of the arm gets as close as possible to a predefined target point.

In the first setting, we assume that the robotic arm is part of a mobile robot that has localised itself near the target. However, due to localisation errors, there is a small possibility that it is near a wall and some joint angles may lead to the arm colliding with the wall and incurring a large cost. Minimising cost entails getting as close to the target as possible while avoiding the region where the wall may be present. The environment variable in this setting is the distance to the wall.

Figures 2(a) and 2(b) show the expected cost (lower is better) of the arm configurations after each timestep for each method. ALOQ, unwarped ALOQ, and RQ-ALOQ greatly outperform the other baselines. Reinforce and TRPO, being relatively sample inefficient, exhibit a very slow rate of improvement in performance, while WSN fails to converge at all.

Figure 2(c) shows the learned arm configurations, as well as the policy that would be learned by ALOQ if there was no wall (No Wall). The shaded region represents the possible locations of the wall. This plot illustrates that ALOQ learns a policy that gets closest to the target. Furthermore, while all the BO based algorithms learn to avoid the wall, active selection of θ𝜃\theta allows ALOQ to do so more quickly: smart quadrature allows it to more efficiently observe rare events and accurately estimate their boundary. For readability we have only presented the arm configurations for algorithms which have performance comparable to ALOQ.

Next we consider a variation in which instead of uncertainty introduced by localisation, some settings of the first joint carry a 5% probability of it breaking, which consequently incurs a large cost. Minimising cost thus entails getting as close to the target as possible, while minimising the probability of the joint breaking.

Figures 3(a) and 3(b) shows the expected cost (lower is better) of the arm configurations after each timestep for each method. Since θ𝜃\theta is continuous in this setting, and WSN requires discrete θ𝜃\theta, it was run on a slightly different version with θ𝜃\theta discretised by 100 equidistant points. The results are similar to the previous experiment, except that the baselines perform worse. In particular, the Naïve baseline, WSN, and Reinforce seem to have converged to a suboptimal policy since they have not witnessed any SREs.

Figure 3(c) shows the learned arm configurations together with the policy that would be learned if there were no SREs (‘No break’). The shaded region represents the joint angles that can lead to failure. This figure illustrates that ALOQ learns a qualitatively different policy than the other algorithms, one which avoids the joint angles that might lead to a breakage while still getting close to the target faster than the other methods. Again for readability we only present the arm configurations for the most competitive algorithms.

Both these baselines are relatively sample inefficient. However, one question that arises is whether these methods eventually find the optimal policy. To check this, we ran them for 2000 iterations with a batch size of 5 trajectories (thus a total of 10000 simulator calls). We repeated this for both the Collision Avoidance and Joint Breakage settings. The expected cost of the arm configurations after each iteration are presented in Figure 4 (we only present the results up to 1000 simulator calls for readability - there is no improvement beyond what can be seen in the plot). Both baselines can solve the tasks in settings without SREs, i.e. where there is no possibility of a collision or a breakage (’No Wall’ and ’No Break’ in the figures). However, in settings with SREs they converge rapidly to a suboptimal policy from which they are unable recover even if run for much longer, since they don’t experience the SREs often enough. This is especially striking in the collision avoidance task where TRPO converges to a policy that has a relatively high probability of leading to a collision.

Now we consider the setting where p​(θ)𝑝𝜃p(\theta) is not known a priori, but must be approximated using trajectories from some baseline policy. In this setting, instead of directly setting the robot arm’s joint angles, we set the torque applied to each joint (π)𝜋(\pi). The final joint angles are determined by the torque and the unknown friction between the joints (θ)𝜃(\theta). Setting the torque too high can lead to the joint breaking, which incurs a large cost.

We use the simulator as a proxy for both real trials as well as the simulated trials. In the first case, we simply sample θ𝜃\theta from a uniform prior, run a baseline policy, and use the observed returns to compute an approximate posterior over θ𝜃\theta. We then use ALOQ to compute the optimal policy over this posterior (‘ALOQ policy’). For comparison, we also compute the MAP of θ𝜃\theta and the corresponding optimal policy (‘MAP policy’). To show that active selection of θ𝜃\theta is advantageous, we also compare against the policy learned by RQ-ALOQ.

Since we are approximating the unknown p​(θ)𝑝𝜃p(\theta) with a set of samples, it makes sense to keep the sample size relatively low for computational efficiency when finding the ALOQ policy (50 samples in this instance). However, to show that ALOQ is robust to this approximation, when comparing the performance of the ALOQ and MAP policies, we used a much larger sample size of 400 for the posterior distribution.

For evaluation, we drew 1000 samples of θ𝜃\theta from the more granular posterior distribution and measured the returns of the three policies for each of the samples. The average cost incurred by the ALOQ policy (presented in Table 1) was 31% lower than that incurred by the MAP policy and 23.6% lower than the RQ-ALOQ policy. This is because ALOQ finds a policy that slightly underperforms the MAP policy in some of cases but avoids over 95% of the SREs (cost ≥\geq70 in Table 1) experienced by the MAP and RQ-ALOQ policies.

As robots move from fully controlled environments to more complex and natural ones, they have to face the inevitable risk of getting damaged. However, it may be expensive or even impossible to decommission a robot whenever any damage condition prevents it from completing its task. Hence, it is desirable to develop methods that enable robots to recover from failure.

Intelligent trial and error (IT&E) (?) has been shown to recover from various damage conditions and thereby prevent catastrophic failure. Before deployment, IT&E uses the simulator to create an archive of diverse and locally high performing policies for the intact robot that are mapped to a lower dimensional behaviour space. If the robot becomes damaged after deployment, it uses BO to quickly find the policy in the archive that has the highest performance on the damaged robot. However, it can only respond after damage has occurred. Though it learns quickly, performance may still be poor while learning during the initial trials after damage occurs. To mitigate this effect, we propose to use ALOQ to learn in simulation the policy with the highest expected performance across the possible damage conditions. By deploying this policy, instead of the policy that is optimal for the intact robot, we can minimise in expectation the negative effects of damage in the period before IT&E has learned to recover.

We consider a hexapod locomotion task with a setup similar to that of (?) to demonstrate this experimentally. The objective is to cross a finish line a fixed distance from its starting point. Failure to cross the line leads to a large negative reward, while the reward for completing the task is inversely proportional to the time taken.

It is possible that a subset of the legs may be damaged or broken when deployed in a physical setting. For our experiments we assume that, based on prior experience, any of the front two or back two legs can be shortened or removed with probability of 10% and 5% respectively, independent of the other legs, leading to 81 possible configurations. We excluded the middle two legs from our experiment as their failure had a relatively lower impact on the hexapod’s movement. The configuration of the six legs acts as our environment variable. Figure 5(a) shows one such setting.

We applied ALOQ to learn the optimal policy given these damage probabilities, but restricted the search to the policies in the archive created by (?). Figure 5(b) shows that ALOQ finds a policy with much higher expected reward than RQ-ALOQ. It also shows the policy that generates the maximum reward when none of the legs are damaged or broken (‘opt undamaged policy’).

To demonstrate that ALOQ learns a policy that can be applied to a physical environment, we also deployed the best ALOQ policy on the real hexapod. In order to limit the number of physical trials required to evaluate ALOQ, we limited the possibility of damage to the rear two legs. The learnt policy performed well on the physical robot because it optimised performance on the rare configurations that matter most for expected return (e.g., either leg shortened).

This paper proposed ALOQ, a novel approach to using BO and BQ to perform sample-efficient RL in a way that is robust to the presence of significant rare events. We empirically evaluated ALOQ on different simulated tasks involving a robotic arm simulator, and a hexapod locomotion taskand showed how it can be also be applied to settings where the distribution of the environment variable is unknown a priori, and that it successfully transfers to a real robot. Our results demonstrated that ALOQ outperforms multiple baselines, including related methods proposed in the literature. Further, ALOQ is computationally efficient and does not require any restrictive assumptions to be made about the environment variables.