# Agentic Multi-Modal QA System ğŸ¤–ğŸ–¼ï¸

A multi-agent system built with **LangGraph** and **LangChain**, capable of answering complex questions about images and text using **Hugging Face models**.  
Each agent performs a distinct role â€” vision analysis, question parsing, knowledge retrieval, and answer generation.

---

## ğŸ§© Architecture
image â†’ Vision Agent â†’ Question Agent â†’ Knowledge Agent â†’ Answer Agent

yaml
Copy code

- **Vision Agent** â€“ analyzes the input image (via Hugging Face vision models).  
- **Question Agent** â€“ interprets user queries and transforms them into reasoning-ready prompts.  
- **Knowledge Agent** â€“ retrieves supporting facts or background info using DuckDuckGo.  
- **Answer Agent** â€“ synthesizes the final answer via an LLM.

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create Environment
Run the following commands to set up the environment:
conda env create -f environment.yml  
conda activate langgraph-env  

### 2ï¸âƒ£ Set Hugging Face Token
You must have a valid Hugging Face token for API access. Export it as an environment variable:
export HF_TOKEN=hf_your_token_here  
export HUGGINGFACEHUB_API_TOKEN=$HF_TOKEN  

### 3ï¸âƒ£ Run the System
Execute the main script to start the pipeline:
python main.py  

The system automatically processes all imageâ€“question pairs in the `inputs/` directory and writes outputs to `output.txt`.

---

## ğŸ§  Notes
- Default model: **HuggingFaceH4/zephyr-7b-alpha**  
- You can switch to smaller, cheaper models (e.g. `mistralai/Mistral-7B-Instruct-v0.2`) in `question_agent.py` or `vision_agent.py`.  
- All `LangChainDeprecationWarning` messages are suppressed in code.  
- The project avoids PyTorch/NVIDIA dependencies and runs fully via **Hugging Face Inference API**.

---

## ğŸ§° Troubleshooting
If you encounter:  
LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated...  
â†’ Safe to ignore (warnings are filtered).  

If you get:  
ValueError: Model not supported for task text-generation...  
â†’ Use a model that supports `text-generation` or `conversational` tasks.


## ğŸ“ Directory Structure

agents/  
&nbsp;&nbsp;â”œâ”€â”€ vision_agent.pyâ€ƒâ€ƒ# Image captioning / understanding  
&nbsp;&nbsp;â”œâ”€â”€ question_agent.pyâ€ƒ# Question interpretation  
&nbsp;&nbsp;â”œâ”€â”€ knowledge_agent.pyâ€ƒ# Web/text retrieval  
&nbsp;&nbsp;â””â”€â”€ answer_agent.pyâ€ƒâ€ƒ# Final synthesis step  

graph.pyâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# Defines LangGraph workflow  
main.pyâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# Entry point  
inputs/â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# Image and question data  
output.txtâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# Generated answers  
environment.ymlâ€ƒâ€ƒ# Conda environment file  
README.mdâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# Project documentation  

---

**Author:** Ashish Kumar  
**Purpose:** Research prototype for multi-modal agentic reasoning using open-source language and vision models.
